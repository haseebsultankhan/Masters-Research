{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a50111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from os.path import join\n",
    "from core.utils import extract, standardize\n",
    "from core.datasets import SeismicDataset1D\n",
    "from torch.utils.data import DataLoader\n",
    "from core.model1D import MustafaNet\n",
    "from sklearn.metrics import r2_score\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "    \n",
    "def preprocess(no_wells, data_flag='seam'):\n",
    "    \"\"\"Function initializes data, performs standardization, and train test split\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    no_wells : int,\n",
    "        number of evenly spaced wells and seismic samples to be evenly sampled \n",
    "        from seismic section.\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    seismic : array_like, shape(num_traces, depth samples)\n",
    "        2-D array containing seismic section \n",
    "        \n",
    "    model : array_like, shape(num_wells, depth samples)\n",
    "        2-D array containing model section \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # get project root directory\n",
    "    project_root = os.getcwd()\n",
    "    \n",
    "    if ~os.path.isdir('data'): # if data directory does not exists then extract\n",
    "        extract('data.zip', project_root)\n",
    "        \n",
    "    if data_flag == 'seam':\n",
    "        # Load data\n",
    "        seismic = np.load(join('data','poststack_seam_seismic.npy')).squeeze()[:, 50:]\n",
    "        seismic = seismic[::2, :]\n",
    "        \n",
    "        # Load targets and standardize data\n",
    "        model = np.load(join('data','seam_elastic_model.npy'))[::3,:,::2][:, :, 50:]\n",
    "        model = model[:,0,:] * model[:,2,:]\n",
    "    \n",
    "    else:\n",
    "        # Load data\n",
    "        seismic = np.load(join('data','marmousi_synthetic_seismic.npy')).squeeze()\n",
    "        model= np.load(join('data', 'marmousi_Ip_model.npy')).squeeze()[::5, ::4]\n",
    "        \n",
    "    \n",
    "    # standardize\n",
    "    seismic, model = standardize(seismic, model, no_wells)\n",
    "    \n",
    "    return seismic, model\n",
    "\n",
    "\n",
    "def train(**kwargs):\n",
    "    \"\"\"Function trains 2-D TCN as specified in the paper\"\"\"\n",
    "    \n",
    "    # obtain data\n",
    "    seismic, model = preprocess(kwargs['no_wells'], kwargs['data_flag'])\n",
    "                                                                           \n",
    "    \n",
    "    # specify pseudolog positions for training and validation\n",
    "    traces_seam_train = np.linspace(0, len(model)-1, kwargs['no_wells'], dtype=int)\n",
    "    traces_seam_validation = np.linspace(0, len(model)-1, 3, dtype=int)\n",
    "    \n",
    "    seam_train_dataset = SeismicDataset1D(seismic, model, traces_seam_train)\n",
    "    seam_train_loader = DataLoader(seam_train_dataset, batch_size = len(seam_train_dataset))\n",
    "    \n",
    "    seam_val_dataset = SeismicDataset1D(seismic, model, traces_seam_validation)\n",
    "    seam_val_loader = DataLoader(seam_val_dataset, batch_size = len(seam_val_dataset))\n",
    "    \n",
    "    \n",
    "    # define device for training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # set up models\n",
    "    model_seam = MustafaNet().to(device)\n",
    "    \n",
    "    # Set up loss\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    optimizer_seam = torch.optim.Adam(model_seam.parameters(),\n",
    "                                      weight_decay=0.0001,\n",
    "                                      lr=0.001)\n",
    "    \n",
    "    # start training \n",
    "    for epoch in range(kwargs['epochs']):\n",
    "    \n",
    "      model_seam.train()\n",
    "      optimizer_seam.zero_grad()\n",
    "      \n",
    "      \n",
    "      for x,y in seam_train_loader:\n",
    "        y_pred = model_seam(x)\n",
    "        loss_train = criterion(y_pred, y) \n",
    "    \n",
    "      for x, y in seam_val_loader:\n",
    "        model_seam.eval()\n",
    "        y_pred = model_seam(x)\n",
    "        val_loss = criterion(y_pred, y)\n",
    "        \n",
    "    \n",
    "      loss_train.backward()\n",
    "      optimizer_seam.step()\n",
    "      \n",
    "      print('Epoch: {} | Train Loss: {:0.4f} | Val Loss: {:0.4f} \\\n",
    "            '.format(epoch, loss_train.item(), val_loss.item()))\n",
    "\n",
    "    \n",
    "    # save trained models\n",
    "    if not os.path.isdir('saved_models'):  # check if directory for saved models exists\n",
    "        os.mkdir('saved_models')\n",
    "        \n",
    "    torch.save(model_seam.state_dict(), 'saved_models/model_seam_1D.pth')\n",
    "\n",
    "def test(**kwargs):\n",
    "    \"\"\"Function tests the trained network on SEAM and Marmousi sections and \n",
    "    prints out the results\"\"\"\n",
    "    \n",
    "    # obtain data\n",
    "    seismic, model = preprocess(kwargs['no_wells'], kwargs['data_flag'])\n",
    "                                                                          \n",
    "    \n",
    "    # define device for training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # specify pseudolog positions for testing \n",
    "    traces_seam_test = np.arange(len(model), dtype=int)\n",
    "    \n",
    "    seam_test_dataset = SeismicDataset1D(seismic, model, traces_seam_test)\n",
    "    seam_test_loader = DataLoader(seam_test_dataset, batch_size = 8)\n",
    "    \n",
    "    # load saved models\n",
    "    if not os.path.isdir('saved_models'):\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), 'saved_models')\n",
    "        \n",
    "    # set up models\n",
    "    model_seam = MustafaNet().to(device)\n",
    "    model_seam.load_state_dict(torch.load('saved_models/model_seam_1D.pth'))\n",
    "    \n",
    "    # infer on SEAM\n",
    "    print(\"\\nInferring ...\")\n",
    "    x, y = seam_test_dataset[0]  # get a sample\n",
    "    AI_pred = torch.zeros((len(seam_test_dataset), y.shape[-1])).float().to(device)\n",
    "    AI_act = torch.zeros((len(seam_test_dataset), y.shape[-1])).float().to(device)\n",
    "    \n",
    "    mem = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(seam_test_loader):\n",
    "          model_seam.eval()\n",
    "          y_pred  = model_seam(x)\n",
    "          AI_pred[mem:mem+len(x)] = y_pred.squeeze().data\n",
    "          AI_act[mem:mem+len(x)] = y.squeeze().data\n",
    "          mem += len(x)\n",
    "          del x, y, y_pred\n",
    "    \n",
    "    vmin, vmax = AI_act.min(), AI_act.max()\n",
    "\n",
    "    AI_pred = AI_pred.detach().cpu().numpy()\n",
    "    AI_act = AI_act.detach().cpu().numpy()\n",
    "    print('r^2 score: {:0.4f}'.format(r2_score(AI_act.T, AI_pred.T)))\n",
    "    print('MSE: {:0.4f}'.format(np.sum((AI_pred-AI_act).ravel()**2)/AI_pred.size))\n",
    "    print('MAE: {:0.4f}'.format(np.sum(np.abs(AI_pred - AI_act)/AI_pred.size)))\n",
    "    print('MedAE: {:0.4f}'.format(np.median(np.abs(AI_pred - AI_act))))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,12))\n",
    "    ax1.imshow(AI_pred.T, vmin=vmin, vmax=vmax, extent=(0,35000,15000,0))\n",
    "    ax1.set_aspect(35/30)\n",
    "    ax1.set_xlabel('Distance Eastimg (m)')\n",
    "    ax1.set_ylabel('Depth (m)')\n",
    "    ax1.set_title('Predicted')\n",
    "    ax2.imshow(AI_act.T, vmin=vmin, vmax=vmax, extent=(0,35000,15000,0))\n",
    "    ax2.set_aspect(35/30)\n",
    "    ax2.set_xlabel('Distance Eastimg (m)')\n",
    "    ax2.set_ylabel('Depth (m)')\n",
    "    ax2.set_title('Ground-Truth')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Hyperparams')\n",
    "    \n",
    "    parser.add_argument('--epochs', nargs='?', type=int, default=900,\n",
    "                        help='Number of epochs. Default = 1000')\n",
    "    parser.add_argument('--no_wells', nargs='?', type=int, default=12,\n",
    "                        help='Number of sampled pseudologs for seismic section. Default = 12.')\n",
    "    parser.add_argument('--data_flag', type=str, default='seam', choices=['seam', 'marmousi'],\n",
    "                        help='Data flag to specify the dataset used to train the model')\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train(no_wells=args.no_wells, epochs=args.epochs, data_flag=args.data_flag)\n",
    "    test(no_wells=args.no_wells, epochs=args.epochs, data_flag=args.data_flag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
